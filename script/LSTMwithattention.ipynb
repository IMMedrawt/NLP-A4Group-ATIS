{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your dataset\n",
    "with open(\"atis.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# with open(\"geography.json\", \"r\") as f:\n",
    "#     raw_data = json.load(f)\n",
    "\n",
    "# Load spaCy tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Structures to store splits and mappings\n",
    "question_split = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "query_split = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "template_pool = set()\n",
    "\n",
    "# Process each item, normalize gold SQLs to create template pool\n",
    "normalized_sqls_per_item = []\n",
    "for item in raw_data:\n",
    "    gold_templates = []\n",
    "    for sql in item[\"sql\"]:\n",
    "        template = sql\n",
    "        for var, val in item[\"sentences\"][0][\"variables\"].items():\n",
    "            template = template.replace(f'\"{val}\"', f'\"@{var}\"')\n",
    "            template = template.replace(val, f'\"@{var}\"')\n",
    "        gold_templates.append(template)\n",
    "        template_pool.add(template)\n",
    "    normalized_sqls_per_item.append((item, gold_templates))\n",
    "\n",
    "template_list = sorted(template_pool)\n",
    "template2id = {t: i for i, t in enumerate(template_list)}\n",
    "id2template = {i: t for t, i in template2id.items()}\n",
    "\n",
    "# Process entries and assign template IDs\n",
    "for (item, gold_templates) in normalized_sqls_per_item:\n",
    "    shortest_sql = sorted(gold_templates, key=lambda x: (len(x), x))[0]\n",
    "    template_id = template2id[shortest_sql]\n",
    "\n",
    "    for sent in item[\"sentences\"]:\n",
    "        raw_text = sent[\"text\"]\n",
    "        variables = sent[\"variables\"]\n",
    "\n",
    "        # Replace placeholders in raw_text for model input\n",
    "        text = raw_text\n",
    "        for var_name, var_value in variables.items():\n",
    "            text = text.replace(var_name, var_value)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = [t.text for t in nlp(text)]\n",
    "\n",
    "        # Create tag sequence\n",
    "        tags = []\n",
    "        for tok in tokens:\n",
    "            matched = False\n",
    "            for var_name, var_value in variables.items():\n",
    "                if tok == var_value or tok in var_value.split():\n",
    "                    tags.append(var_name)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                tags.append(\"O\")\n",
    "\n",
    "        # Fill in gold SQL (for seq2seq generation target)\n",
    "        gold_sql = shortest_sql\n",
    "        for var_name, var_value in variables.items():\n",
    "            gold_sql = gold_sql.replace(f\"@{var_name}\", f'\"{var_value}\"')\n",
    "\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"template_id\": template_id,\n",
    "            \"template_sql\": gold_templates,\n",
    "            \"variables\": variables,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"tokens\": tokens,\n",
    "            \"tags\": tags,\n",
    "            \"gold_sql\": gold_sql.strip(),\n",
    "        }\n",
    "\n",
    "        # Add to split\n",
    "        question_split[sent[\"question-split\"]].append(entry)\n",
    "        query_split[item[\"query-split\"]].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "tag_counter = Counter()\n",
    "\n",
    "for entry in question_split[\"train\"]:  \n",
    "    text = entry[\"text\"]\n",
    "    tokens = [t.text for t in nlp(text)]\n",
    "\n",
    "    tags = []\n",
    "    for tok in tokens:\n",
    "        matched = False\n",
    "        for var_name, var_val in entry[\"variables\"].items():\n",
    "            if tok == var_val or tok in var_val.split():\n",
    "                tags.append(var_name)\n",
    "                tag_counter[var_name] += 1\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            tags.append(\"O\")\n",
    "            tag_counter[\"O\"] += 1\n",
    "\n",
    "    token_counter.update(tokens)\n",
    "    entry[\"tokens\"] = tokens\n",
    "    entry[\"tags\"] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for split in [\"dev\", \"test\"]:\n",
    "    for entry in question_split[split]:\n",
    "        text = entry[\"text\"]\n",
    "        tokens = [t.text for t in nlp(text)]\n",
    "        tags = []\n",
    "        for tok in tokens:\n",
    "            matched = False\n",
    "            for var_name, var_val in entry[\"variables\"].items():\n",
    "                if tok == var_val or tok in var_val.split():\n",
    "                    tags.append(var_name)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                tags.append(\"O\")\n",
    "        entry[\"tokens\"] = tokens\n",
    "        entry[\"tags\"] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Token vocab\n",
    "token2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for token in token_counter:\n",
    "    token2id[token] = len(token2id)\n",
    "\n",
    "# Tag vocab \n",
    "PAD_TAG = \"<PAD>\"\n",
    "tag_counter[PAD_TAG] = 1  \n",
    "\n",
    "# tag2id = {}\n",
    "# for tag in tag_counter:\n",
    "#     tag2id[tag] = len(tag2id)\n",
    "\n",
    "tag2id = {\"<PAD>\": 0, \"O\": 1}\n",
    "for tag in tag_counter:\n",
    "    if tag not in tag2id:\n",
    "        tag2id[tag] = len(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, token2id, sql_token2id):\n",
    "        self.data = data\n",
    "        self.token2id = token2id\n",
    "        self.sql_token2id = sql_token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        # Encoder input\n",
    "        token_ids = [self.token2id.get(tok, self.token2id[\"<UNK>\"]) for tok in entry[\"tokens\"]]\n",
    "        # Decoder target (shortest SQL template filled with values)\n",
    "        sql = sorted(entry[\"template_sql\"], key=lambda x: (len(x), x))[0]\n",
    "        for var, val in entry[\"variables\"].items():\n",
    "            sql = sql.replace(f\"@{var}\", f'\"{val}\"')\n",
    "        sql_tokens = re.findall(r\"\\w+|[^\\s\\w]\", sql)\n",
    "        target_ids = [self.sql_token2id.get(tok, self.sql_token2id[\"<UNK>\"]) for tok in sql_tokens]\n",
    "\n",
    "        return {\n",
    "            \"token_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def pad_seq2seq_batch(batch):\n",
    "    max_input_len = max(len(item[\"token_ids\"]) for item in batch)\n",
    "    max_target_len = max(len(item[\"target_ids\"]) for item in batch)\n",
    "\n",
    "    for item in batch:\n",
    "        item[\"token_ids\"] = F.pad(item[\"token_ids\"], (0, max_input_len - len(item[\"token_ids\"])), value=token2id[\"<PAD>\"])\n",
    "        item[\"target_ids\"] = F.pad(item[\"target_ids\"], (0, max_target_len - len(item[\"target_ids\"])), value=sql_token2id[\"<PAD>\"])\n",
    "\n",
    "    token_ids = torch.stack([item[\"token_ids\"] for item in batch])\n",
    "    target_ids = torch.stack([item[\"target_ids\"] for item in batch])\n",
    "    return token_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Special tokens\n",
    "SQL_PAD = \"<PAD>\"\n",
    "SQL_UNK = \"<UNK>\"\n",
    "SQL_START = \"<START>\"\n",
    "SQL_END = \"<END>\"\n",
    "\n",
    "# Step 1: Collect all SQL tokens from training set\n",
    "sql_token_counter = Counter()\n",
    "\n",
    "for entry in question_split[\"train\"]:\n",
    "    sql = sorted(entry[\"template_sql\"], key=lambda x: (len(x), x))[0]\n",
    "    for var, val in entry[\"variables\"].items():\n",
    "        sql = sql.replace(f\"@{var}\", f'\"{val}\"')\n",
    "    # Basic tokenization: split by words and punctuation\n",
    "    sql_tokens = re.findall(r\"\\w+|[^\\s\\w]\", sql)\n",
    "    sql_token_counter.update(sql_tokens)\n",
    "\n",
    "# Step 2: Create vocab dict\n",
    "sql_token2id = {\n",
    "    SQL_PAD: 0,\n",
    "    SQL_UNK: 1,\n",
    "    SQL_START: 2,\n",
    "    SQL_END: 3\n",
    "}\n",
    "\n",
    "for token in sql_token_counter:\n",
    "    sql_token2id[token] = len(sql_token2id)\n",
    "\n",
    "# Step 3: Reverse map for inference (optional)\n",
    "id2sql_token = {i: tok for tok, i in sql_token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_train_set = Seq2SeqDataset(question_split[\"train\"], token2id, sql_token2id)\n",
    "seq2seq_dev_set = Seq2SeqDataset(question_split[\"dev\"], token2id, sql_token2id)\n",
    "seq2seq_test_set = Seq2SeqDataset(question_split[\"test\"], token2id, sql_token2id)\n",
    "\n",
    "seq2seq_train_loader = DataLoader(seq2seq_train_set, batch_size=32, shuffle=True, collate_fn=pad_seq2seq_batch)\n",
    "seq2seq_dev_loader = DataLoader(seq2seq_dev_set, batch_size=32, collate_fn=pad_seq2seq_batch)\n",
    "seq2seq_test_loader = DataLoader(seq2seq_test_set, batch_size=32, collate_fn=pad_seq2seq_batch)\n",
    "seq2seq_query_test_set = Seq2SeqDataset(query_split[\"test\"], token2id, sql_token2id)\n",
    "seq2seq_query_test_loader = DataLoader(seq2seq_query_test_set, batch_size=32, collate_fn=pad_seq2seq_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2id[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq, lengths):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed)\n",
    "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        return encoder_outputs, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=sql_token2id[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward_step(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_token)  # (B, 1, E)\n",
    "\n",
    "        # Attention: compute dot-product attention weights\n",
    "        h_t = hidden[0][-1]  # (B, H)\n",
    "        attn_scores = torch.bmm(encoder_outputs, h_t.unsqueeze(2)).squeeze(2)  # (B, T)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # (B, T)\n",
    "\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (B, 1, H)\n",
    "        lstm_input = torch.cat([embedded, context], dim=2)  # (B, 1, E+H)\n",
    "\n",
    "        output, hidden = self.lstm(lstm_input, hidden)  # output: (B, 1, H)\n",
    "        output = self.out(output)  # (B, 1, V)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def forward(self, trg, hidden, encoder_outputs):\n",
    "        outputs = []\n",
    "        input_token = trg[:, 0].unsqueeze(1)  # <START>\n",
    "\n",
    "        for t in range(1, trg.size(1)):\n",
    "            output, hidden, _ = self.forward_step(input_token, hidden, encoder_outputs)\n",
    "            outputs.append(output)\n",
    "            input_token = trg[:, t].unsqueeze(1)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (B, T-1, V)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len - 1, vocab_size).to(self.device)\n",
    "\n",
    "        lengths = (src != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "        embedded = self.encoder.embedding(src)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h, c) = self.encoder.lstm(packed)\n",
    "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        output = self.decoder(trg, (h, c), encoder_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = LSTMEncoder(len(token2id), embedding_dim=100, hidden_dim=128)\n",
    "decoder = LSTMDecoderWithAttention(len(sql_token2id), embedding_dim=100, hidden_dim=128)\n",
    "model = Seq2SeqWithAttention(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, loader, loss_fn, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for token_ids, target_ids in loader:\n",
    "        token_ids = token_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_input = target_ids[:, :-1]\n",
    "        decoder_target = target_ids[:, 1:]\n",
    "\n",
    "        output = model(token_ids, decoder_input)  # [B, T-1, V]\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        target = decoder_target.reshape(-1)\n",
    "\n",
    "        # Sanity check: prevent shape mismatch crash\n",
    "        min_len = min(output.size(0), target.size(0))\n",
    "        loss = loss_fn(output[:min_len], target[:min_len])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_seq2seq(model, loader, loss_fn, epoch, name=\"Dev\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token_ids, target_ids in loader:\n",
    "            token_ids = token_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            decoder_input = target_ids[:, :-1]\n",
    "            decoder_target = target_ids[:, 1:]\n",
    "\n",
    "            output = model(token_ids, decoder_input)  # [B, T-1, V]\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            target = decoder_target.reshape(-1)\n",
    "\n",
    "            min_len = min(output.size(0), target.size(0))\n",
    "            loss = loss_fn(output[:min_len], target[:min_len])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = output.argmax(dim=-1)\n",
    "            correct += (pred[:min_len] == target[:min_len]).sum().item()\n",
    "            total += min_len\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"{name} Epoch {epoch}: Loss={avg_loss:.4f}, Accuracy={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=3.5441\n",
      "Dev Epoch 1: Loss=3.1747, Accuracy=0.0833\n",
      "Epoch 2: Loss=3.0440\n",
      "Dev Epoch 2: Loss=3.0186, Accuracy=0.0824\n",
      "Epoch 3: Loss=2.9198\n",
      "Dev Epoch 3: Loss=2.9331, Accuracy=0.0823\n",
      "Epoch 4: Loss=2.8472\n",
      "Dev Epoch 4: Loss=2.8838, Accuracy=0.0825\n",
      "Epoch 5: Loss=2.7977\n",
      "Dev Epoch 5: Loss=2.8429, Accuracy=0.0828\n",
      "Epoch 6: Loss=2.7571\n",
      "Dev Epoch 6: Loss=2.8146, Accuracy=0.0822\n",
      "Epoch 7: Loss=2.7238\n",
      "Dev Epoch 7: Loss=2.7928, Accuracy=0.0830\n",
      "Epoch 8: Loss=2.6978\n",
      "Dev Epoch 8: Loss=2.7715, Accuracy=0.0829\n",
      "Epoch 9: Loss=2.6848\n",
      "Dev Epoch 9: Loss=2.7579, Accuracy=0.0832\n",
      "Epoch 10: Loss=2.6646\n",
      "Dev Epoch 10: Loss=2.7463, Accuracy=0.0833\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=sql_token2id[\"<PAD>\"])\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_seq2seq(model, seq2seq_train_loader, loss_fn, optimizer, epoch)\n",
    "    validate_seq2seq(model, seq2seq_dev_loader, loss_fn, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_sql(s):\n",
    "    return \" \".join(s.strip().split()).lower()\n",
    "\n",
    "def generate_sql_from_model_with_attention(model, loader, output_file=\"seq2seq_attn_test_predictions.txt\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    golds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            for token_ids, target_ids in loader:\n",
    "                token_ids = token_ids.to(device)\n",
    "                lengths = (token_ids != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "                batch_size = token_ids.size(0)\n",
    "\n",
    "                # Encode\n",
    "                embedded = model.encoder.embedding(token_ids)\n",
    "                packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "                packed_output, (h, c) = model.encoder.lstm(packed)\n",
    "                encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "                # Decode\n",
    "                input_token = torch.full((batch_size, 1), sql_token2id[\"<START>\"], dtype=torch.long, device=device)\n",
    "                outputs = []\n",
    "\n",
    "                for _ in range(100):\n",
    "                    output, (h, c), _ = model.decoder.forward_step(input_token, (h, c), encoder_outputs)\n",
    "                    logits = output[:, -1, :]\n",
    "                    pred_token = logits.argmax(dim=-1, keepdim=True)\n",
    "                    outputs.append(pred_token)\n",
    "                    input_token = pred_token\n",
    "\n",
    "                output_seqs = torch.cat(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_tokens = output_seqs[i].tolist()\n",
    "                    if sql_token2id[\"<END>\"] in pred_tokens:\n",
    "                        pred_tokens = pred_tokens[:pred_tokens.index(sql_token2id[\"<END>\"])]\n",
    "                    pred_sql = normalize_sql(\" \".join(id2sql_token[tok] for tok in pred_tokens))\n",
    "\n",
    "                    gold_tokens = target_ids[i].tolist()\n",
    "                    if sql_token2id[\"<END>\"] in gold_tokens:\n",
    "                        gold_tokens = gold_tokens[:gold_tokens.index(sql_token2id[\"<END>\"])]\n",
    "                    gold_sql = normalize_sql(\" \".join(id2sql_token[tok] for tok in gold_tokens))\n",
    "\n",
    "                    predictions.append(pred_sql)\n",
    "                    golds.append(gold_sql)\n",
    "                    f.write(pred_sql + \"\\n\")\n",
    "\n",
    "                    if pred_sql == gold_sql:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Test Seq2Seq+Attention] SQL Generation Accuracy: {acc:.4f}\")\n",
    "    return predictions, golds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation on Question Split]\n",
      "[Test Seq2Seq+Attention] SQL Generation Accuracy: 0.0000\n",
      "[Evaluation on Query Split]\n",
      "[Test Seq2Seq+Attention] SQL Generation Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"[Evaluation on Question Split]\")\n",
    "preds_question_attention, golds_question_attention = generate_sql_from_model_with_attention(\n",
    "    model, \n",
    "    seq2seq_test_loader, \n",
    "    output_file=\"seq2seq_question_preds.txt\"\n",
    ")\n",
    "\n",
    "print(\"[Evaluation on Query Split]\")\n",
    "preds_query_attention, golds_query_attention = generate_sql_from_model_with_attention(\n",
    "    model, \n",
    "    seq2seq_query_test_loader,  \n",
    "    output_file=\"seq2seq_query_preds.txt\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
