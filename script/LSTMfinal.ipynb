{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ATIS JSON data\n",
    "with open(\"atis.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# with open(\"geography.json\", \"r\") as f:\n",
    "#     raw_data = json.load(f)\n",
    "\n",
    "# Load spaCy tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Structures to store splits and mappings\n",
    "question_split = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "query_split = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "template_pool = set()\n",
    "\n",
    "# Process each item, normalize gold SQLs to create template pool\n",
    "normalized_sqls_per_item = []\n",
    "for item in raw_data:\n",
    "    gold_templates = []\n",
    "    for sql in item[\"sql\"]:\n",
    "        # Replace all variable values with @var\n",
    "        template = sql\n",
    "        for var, val in item[\"sentences\"][0][\"variables\"].items():\n",
    "            template = template.replace(f'\"{val}\"', f'\"@{var}\"')\n",
    "            template = template.replace(val, f'\"@{var}\"')\n",
    "        gold_templates.append(template)\n",
    "        template_pool.add(template)\n",
    "    normalized_sqls_per_item.append((item, gold_templates))\n",
    "\n",
    "\n",
    "template_list = sorted(template_pool)\n",
    "template2id = {t: i for i, t in enumerate(template_list)}\n",
    "id2template = {i: t for t, i in template2id.items()}\n",
    "\n",
    "# Process entries and assign template IDs\n",
    "for (item, gold_templates) in normalized_sqls_per_item:\n",
    "    # Use shortest template to assign template_id\n",
    "    shortest_sql = sorted(gold_templates, key=lambda x: (len(x), x))[0]\n",
    "    template_id = template2id[shortest_sql]\n",
    "\n",
    "    for sent in item[\"sentences\"]:\n",
    "        raw_text = sent[\"text\"]\n",
    "        variables = sent[\"variables\"]\n",
    "\n",
    "        # Replace placeholders in raw_text for model input\n",
    "        text = raw_text\n",
    "        for var_name, var_value in variables.items():\n",
    "            text = text.replace(var_name, var_value)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = [t.text for t in nlp(text)]\n",
    "        raw_tokens = [t.text for t in nlp(raw_text)]\n",
    "\n",
    "        # Create tag sequence\n",
    "        tags = []\n",
    "        for tok in raw_tokens:\n",
    "            if tok in variables:\n",
    "                tags.append(tok)\n",
    "            else:\n",
    "                tags.append(\"O\")\n",
    "\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"template_id\": template_id,\n",
    "            \"template_sql\": gold_templates,\n",
    "            \"variables\": variables,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"tokens\": tokens,\n",
    "            \"tags\": tags,\n",
    "        }\n",
    "\n",
    "        # Add to split\n",
    "        question_split[sent[\"question-split\"]].append(entry)\n",
    "        query_split[item[\"query-split\"]].append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "tag_counter = Counter()\n",
    "\n",
    "# Go through all splits for both question and query versions\n",
    "for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "    for entry in question_split[split_name]:\n",
    "        raw_text = entry[\"raw_text\"]\n",
    "        tokens = [t.text for t in nlp(raw_text)]\n",
    "\n",
    "        # Generate tags: placeholder tokens get their name, others are \"O\"\n",
    "        tags = []\n",
    "        for tok in tokens:\n",
    "            if tok in entry[\"variables\"]:\n",
    "                tags.append(tok)\n",
    "                tag_counter[tok] += 1\n",
    "            else:\n",
    "                tags.append(\"O\")\n",
    "                tag_counter[\"O\"] += 1\n",
    "        token_counter.update(tokens)\n",
    "\n",
    "        # Store tokenized version and tags for use later\n",
    "        entry[\"tokens\"] = tokens\n",
    "        entry[\"tags\"] = tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Token vocab\n",
    "token2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for token in token_counter:\n",
    "    token2id[token] = len(token2id)\n",
    "\n",
    "# Tag vocab \n",
    "PAD_TAG = \"<PAD>\"\n",
    "tag_counter[PAD_TAG] = 1  \n",
    "\n",
    "tag2id = {}\n",
    "for tag in tag_counter:\n",
    "    tag2id[tag] = len(tag2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATISDataset(Dataset):\n",
    "    def __init__(self, data, token2id, tag2id):\n",
    "        self.data = data\n",
    "        self.token2id = token2id\n",
    "        self.tag2id = tag2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        token_ids = [self.token2id.get(tok, self.token2id[\"<UNK>\"]) for tok in entry[\"tokens\"]]\n",
    "        tag_ids = [self.tag2id[tag] for tag in entry[\"tags\"]]\n",
    "        template_id = entry[\"template_id\"]\n",
    "\n",
    "        return {\n",
    "            \"token_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"tag_ids\": torch.tensor(tag_ids, dtype=torch.long),\n",
    "            \"template_id\": torch.tensor(template_id, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "def pad_batch(batch):\n",
    "    # Find max length in batch\n",
    "    max_len = max(len(item[\"token_ids\"]) for item in batch)\n",
    "\n",
    "    # Pad token and tag sequences\n",
    "    for item in batch:\n",
    "        pad_len = max_len - len(item[\"token_ids\"])\n",
    "        item[\"token_ids\"] = F.pad(item[\"token_ids\"], (0, pad_len), value=token2id[\"<PAD>\"])\n",
    "        item[\"tag_ids\"] = F.pad(item[\"tag_ids\"], (0, pad_len), value=tag2id[\"<PAD>\"])\n",
    "    # Stack tensors\n",
    "    token_ids = torch.stack([item[\"token_ids\"] for item in batch])\n",
    "    tag_ids = torch.stack([item[\"tag_ids\"] for item in batch])\n",
    "    template_ids = torch.stack([item[\"template_id\"] for item in batch])\n",
    "\n",
    "    return token_ids, tag_ids, template_ids\n",
    "\n",
    "train_set = ATISDataset(question_split[\"train\"], token2id, tag2id)\n",
    "dev_set = ATISDataset(question_split[\"dev\"], token2id, tag2id)\n",
    "test_set = ATISDataset(question_split[\"test\"], token2id, tag2id)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=pad_batch)\n",
    "dev_loader = DataLoader(dev_set, batch_size=32, collate_fn=pad_batch)\n",
    "test_loader = DataLoader(test_set, batch_size=32, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build default values for each variable type (from training data)\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "default_values = defaultdict(str)\n",
    "var_value_counts = defaultdict(Counter)\n",
    "\n",
    "for entry in question_split[\"train\"]:\n",
    "    for var_name, var_val in entry[\"variables\"].items():\n",
    "        var_value_counts[var_name][var_val] += 1\n",
    "\n",
    "for var_name, counter in var_value_counts.items():\n",
    "    default_values[var_name] = counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifierTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, template_size, embedding_dim=100, hidden_dim=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2id[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.tag_fc = nn.Linear(hidden_dim, tag_size)\n",
    "        self.template_fc = nn.Linear(hidden_dim, template_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        hn = self.dropout(hn[-1])  \n",
    "\n",
    "        tag_logits = self.tag_fc(output)\n",
    "        template_logits = self.template_fc(hn)\n",
    "\n",
    "        return tag_logits, template_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMClassifierTagger(\n",
    "    vocab_size=len(token2id),\n",
    "    tag_size=len(tag2id),\n",
    "    template_size=len(template2id),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, tag_loss_fn, template_loss_fn, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_template_correct = 0\n",
    "    total_tag_correct = 0\n",
    "    total_tokens = 0\n",
    "    for token_ids, tag_ids, template_ids in loader:\n",
    "        token_ids = token_ids.to(device)\n",
    "        tag_ids = tag_ids.to(device)\n",
    "        template_ids = template_ids.to(device)\n",
    "\n",
    "        lengths = (token_ids != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tag_logits, template_logits = model(token_ids, lengths)\n",
    "\n",
    "        # Tag loss\n",
    "        tag_logits_reshaped = tag_logits.view(-1, tag_logits.shape[-1])\n",
    "        tag_ids_reshaped = tag_ids.view(-1)\n",
    "        tag_loss = tag_loss_fn(tag_logits_reshaped, tag_ids_reshaped)\n",
    "\n",
    "        # Template loss\n",
    "        template_loss = template_loss_fn(template_logits, template_ids)\n",
    "\n",
    "        loss = tag_loss + template_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracies\n",
    "        tag_preds = tag_logits.argmax(dim=-1)\n",
    "        mask = tag_ids != tag2id[\"<PAD>\"]\n",
    "        total_tag_correct += ((tag_preds == tag_ids) & mask).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "        template_preds = template_logits.argmax(dim=-1)\n",
    "        total_template_correct += (template_preds == template_ids).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    tag_acc = total_tag_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    template_acc = total_template_correct / len(loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, TagAcc={tag_acc:.4f}, TemplateAcc={template_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, tag_loss_fn, template_loss_fn, epoch, name=\"Dev\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_template_correct = 0\n",
    "    total_tag_correct = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for token_ids, tag_ids, template_ids in loader:\n",
    "            token_ids = token_ids.to(device)\n",
    "            tag_ids = tag_ids.to(device)\n",
    "            template_ids = template_ids.to(device)\n",
    "\n",
    "            lengths = (token_ids != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "            tag_logits, template_logits = model(token_ids, lengths)\n",
    "\n",
    "            tag_logits_reshaped = tag_logits.view(-1, tag_logits.shape[-1])\n",
    "            tag_ids_reshaped = tag_ids.view(-1)\n",
    "            tag_loss = tag_loss_fn(tag_logits_reshaped, tag_ids_reshaped)\n",
    "            template_loss = template_loss_fn(template_logits, template_ids)\n",
    "\n",
    "            loss = tag_loss + template_loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            tag_preds = tag_logits.argmax(dim=-1)\n",
    "            mask = tag_ids != tag2id[\"<PAD>\"]\n",
    "            total_tag_correct += ((tag_preds == tag_ids) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "            template_preds = template_logits.argmax(dim=-1)\n",
    "            total_template_correct += (template_preds == template_ids).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    tag_acc = total_tag_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    template_acc = total_template_correct / len(loader.dataset)\n",
    "\n",
    "    print(f\"{name} Epoch {epoch}: Loss={avg_loss:.4f}, TagAcc={tag_acc:.4f}, TemplateAcc={template_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "tag_loss_fn = nn.CrossEntropyLoss(ignore_index=tag2id[\"<PAD>\"])\n",
    "template_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top templates in train: [(223, 28), (4, 28), (171, 24), (55, 20), (41, 17)]\n",
      "Top templates in test: [(223, 12), (4, 12), (55, 11), (204, 11), (41, 9)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_templates = [entry[\"template_id\"] for entry in question_split[\"train\"]]\n",
    "test_templates = [entry[\"template_id\"] for entry in question_split[\"test\"]]\n",
    "\n",
    "print(\"Top templates in train:\", Counter(train_templates).most_common(5))\n",
    "print(\"Top templates in test:\", Counter(test_templates).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=7.2729, TagAcc=0.7068, TemplateAcc=0.0801\n",
      "Dev Epoch 1: Loss=6.1550, TagAcc=0.9149, TemplateAcc=0.1224\n",
      "Epoch 2: Loss=5.4033, TagAcc=0.9003, TemplateAcc=0.1129\n",
      "Dev Epoch 2: Loss=5.3423, TagAcc=0.9176, TemplateAcc=0.1224\n",
      "Epoch 3: Loss=4.5678, TagAcc=0.9541, TemplateAcc=0.1803\n",
      "Dev Epoch 3: Loss=5.0064, TagAcc=0.9707, TemplateAcc=0.2449\n",
      "Epoch 4: Loss=4.1134, TagAcc=0.9664, TemplateAcc=0.2240\n",
      "Dev Epoch 4: Loss=4.7865, TagAcc=0.9707, TemplateAcc=0.2449\n",
      "Epoch 5: Loss=3.7838, TagAcc=0.9681, TemplateAcc=0.2532\n",
      "Dev Epoch 5: Loss=4.5137, TagAcc=0.9761, TemplateAcc=0.2857\n",
      "Epoch 6: Loss=3.4023, TagAcc=0.9740, TemplateAcc=0.3698\n",
      "Dev Epoch 6: Loss=4.2255, TagAcc=0.9867, TemplateAcc=0.3878\n",
      "Epoch 7: Loss=3.1197, TagAcc=0.9826, TemplateAcc=0.4080\n",
      "Dev Epoch 7: Loss=4.0687, TagAcc=0.9894, TemplateAcc=0.3878\n",
      "Epoch 8: Loss=2.8276, TagAcc=0.9889, TemplateAcc=0.4809\n",
      "Dev Epoch 8: Loss=3.8619, TagAcc=0.9947, TemplateAcc=0.4286\n",
      "Epoch 9: Loss=2.5436, TagAcc=0.9917, TemplateAcc=0.5392\n",
      "Dev Epoch 9: Loss=3.7605, TagAcc=0.9973, TemplateAcc=0.4490\n",
      "Epoch 10: Loss=2.3642, TagAcc=0.9902, TemplateAcc=0.5756\n",
      "Dev Epoch 10: Loss=3.6252, TagAcc=0.9973, TemplateAcc=0.4490\n",
      "Epoch 11: Loss=2.0204, TagAcc=0.9936, TemplateAcc=0.6321\n",
      "Dev Epoch 11: Loss=3.4629, TagAcc=0.9973, TemplateAcc=0.4694\n",
      "Epoch 12: Loss=1.8628, TagAcc=0.9931, TemplateAcc=0.6521\n",
      "Dev Epoch 12: Loss=3.4049, TagAcc=0.9973, TemplateAcc=0.5306\n",
      "Epoch 13: Loss=1.7160, TagAcc=0.9939, TemplateAcc=0.6831\n",
      "Dev Epoch 13: Loss=3.3067, TagAcc=1.0000, TemplateAcc=0.5306\n",
      "Epoch 14: Loss=1.5240, TagAcc=0.9948, TemplateAcc=0.7122\n",
      "Dev Epoch 14: Loss=3.2496, TagAcc=1.0000, TemplateAcc=0.5102\n",
      "Epoch 15: Loss=1.4133, TagAcc=0.9934, TemplateAcc=0.7450\n",
      "Dev Epoch 15: Loss=3.1469, TagAcc=1.0000, TemplateAcc=0.5306\n",
      "Epoch 16: Loss=1.2515, TagAcc=0.9951, TemplateAcc=0.7668\n",
      "Dev Epoch 16: Loss=3.0972, TagAcc=1.0000, TemplateAcc=0.5306\n",
      "Epoch 17: Loss=1.1721, TagAcc=0.9944, TemplateAcc=0.8015\n",
      "Dev Epoch 17: Loss=3.0652, TagAcc=1.0000, TemplateAcc=0.5306\n",
      "Epoch 18: Loss=1.0908, TagAcc=0.9953, TemplateAcc=0.8379\n",
      "Dev Epoch 18: Loss=3.0116, TagAcc=1.0000, TemplateAcc=0.5306\n",
      "Epoch 19: Loss=0.9140, TagAcc=0.9963, TemplateAcc=0.8634\n",
      "Dev Epoch 19: Loss=2.9548, TagAcc=1.0000, TemplateAcc=0.5510\n",
      "Epoch 20: Loss=0.8403, TagAcc=0.9956, TemplateAcc=0.8980\n",
      "Dev Epoch 20: Loss=2.9224, TagAcc=1.0000, TemplateAcc=0.5510\n",
      "Epoch 21: Loss=0.7352, TagAcc=0.9966, TemplateAcc=0.9180\n",
      "Dev Epoch 21: Loss=2.9047, TagAcc=1.0000, TemplateAcc=0.5510\n",
      "Epoch 22: Loss=0.7011, TagAcc=0.9961, TemplateAcc=0.9253\n",
      "Dev Epoch 22: Loss=2.8613, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 23: Loss=0.6081, TagAcc=0.9973, TemplateAcc=0.9381\n",
      "Dev Epoch 23: Loss=2.8467, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 24: Loss=0.5550, TagAcc=0.9971, TemplateAcc=0.9581\n",
      "Dev Epoch 24: Loss=2.8561, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 25: Loss=0.5204, TagAcc=0.9971, TemplateAcc=0.9727\n",
      "Dev Epoch 25: Loss=2.8555, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 26: Loss=0.4641, TagAcc=0.9971, TemplateAcc=0.9745\n",
      "Dev Epoch 26: Loss=2.8045, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 27: Loss=0.4012, TagAcc=0.9983, TemplateAcc=0.9818\n",
      "Dev Epoch 27: Loss=2.8068, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 28: Loss=0.3620, TagAcc=0.9978, TemplateAcc=0.9818\n",
      "Dev Epoch 28: Loss=2.7698, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 29: Loss=0.3500, TagAcc=0.9975, TemplateAcc=0.9909\n",
      "Dev Epoch 29: Loss=2.7912, TagAcc=1.0000, TemplateAcc=0.5714\n",
      "Epoch 30: Loss=0.3303, TagAcc=0.9978, TemplateAcc=0.9909\n",
      "Dev Epoch 30: Loss=2.7512, TagAcc=1.0000, TemplateAcc=0.5918\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 31):\n",
    "    train(model, train_loader, optimizer, tag_loss_fn, template_loss_fn, epoch)\n",
    "    validate(model, dev_loader, tag_loss_fn, template_loss_fn, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2template = {i: t for i, t in enumerate(template_list)}  \n",
    "id2tag = {v: k for k, v in tag2id.items()}\n",
    "id2token = {v: k for k, v in token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def fill_variables(template, var_map):\n",
    "    sql = template\n",
    "    for var, val in var_map.items():\n",
    "        sql = re.sub(rf'@{re.escape(var)}\\b', f'\"{val}\"', sql)\n",
    "    return sql\n",
    "\n",
    "def normalize_sql(s):\n",
    "    return \" \".join(s.strip().split()).lower()\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "def tag_to_var(tag):\n",
    "    return tag.lstrip(\"B-\").lstrip(\"I-\").lower()\n",
    "\n",
    "def evaluate(model, loader, dataset_split, output_file=\"predicted_sql.txt\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    template_correct = 0\n",
    "    tag_correct = 0\n",
    "    tag_total = 0\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            for i, (token_ids, tag_ids, template_ids) in enumerate(loader):\n",
    "                token_ids = token_ids.to(device)\n",
    "                lengths = (token_ids != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "                tag_logits, template_logits = model(token_ids, lengths)\n",
    "\n",
    "                tag_preds = tag_logits.argmax(dim=-1).cpu().numpy()\n",
    "                template_preds = template_logits.argmax(dim=-1).cpu().numpy()\n",
    "                token_ids = token_ids.cpu().numpy()\n",
    "\n",
    "                for b in range(token_ids.shape[0]):\n",
    "                    pred_tags = tag_preds[b]\n",
    "                    tokens = simple_tokenize(dataset_split[total][\"text\"])[:len(pred_tags)]\n",
    "                    pred_template_id = template_preds[b]\n",
    "                    sql_template = id2template[pred_template_id]\n",
    "\n",
    "                    # Template classification accuracy\n",
    "                    if pred_template_id == dataset_split[total][\"template_id\"]:\n",
    "                        template_correct += 1\n",
    "\n",
    "                    # Tagging accuracy\n",
    "                    gold_tags = [tag2id[tag] for tag in dataset_split[total][\"tags\"]]\n",
    "                    min_len = min(len(gold_tags), len(pred_tags))\n",
    "                    tag_correct += sum(1 for i in range(min_len) if pred_tags[i] == gold_tags[i])\n",
    "                    tag_total += min_len\n",
    "\n",
    "                    # Extract predicted variables\n",
    "                    var_map = {}\n",
    "                    i = 0\n",
    "                    while i < len(tokens) and i < len(pred_tags):\n",
    "                        tag = id2tag[pred_tags[i]]\n",
    "                        token = tokens[i]\n",
    "\n",
    "                        if tag != \"O\":\n",
    "                            var_name = tag_to_var(tag)\n",
    "                            merged_tokens = [token]\n",
    "                            j = i + 1\n",
    "                            while j < len(tokens):\n",
    "                                if id2tag[pred_tags[j]] != \"O\":\n",
    "                                    break\n",
    "                                if tokens[j][0].isupper():\n",
    "                                    merged_tokens.append(tokens[j])\n",
    "                                    j += 1\n",
    "                                else:\n",
    "                                    break\n",
    "                            if var_name not in var_map:\n",
    "                                var_map[var_name] = \" \".join(merged_tokens)\n",
    "                            i = j\n",
    "                        else:\n",
    "                            i += 1\n",
    "\n",
    "                    # Fill in the template using predicted variables\n",
    "                    filled_sql = fill_variables(sql_template, {\n",
    "                        var: var_map.get(var, default_values[var]) for var in default_values\n",
    "                    })\n",
    "\n",
    "                    f.write(filled_sql.strip() + \"\\n\")\n",
    "\n",
    "                    # Evaluate match with any correct SQL\n",
    "                    example = dataset_split[total]\n",
    "                    valid_sqls = []\n",
    "                    for sql in example[\"template_sql\"]:\n",
    "                        if isinstance(sql, list):\n",
    "                            sql = \" \".join(sql) if all(len(tok) > 1 for tok in sql) else \"\".join(sql)\n",
    "                        elif not isinstance(sql, str):\n",
    "                            sql = str(sql)\n",
    "                        sql_filled = sql\n",
    "                        for var, val in example[\"variables\"].items():\n",
    "                            sql_filled = re.sub(rf'@{re.escape(var)}\\b', f'\"{val}\"', sql_filled)\n",
    "                        valid_sqls.append(sql_filled.strip())\n",
    "\n",
    "                    pred_norm = normalize_sql(filled_sql)\n",
    "                    valid_norms = [normalize_sql(s) for s in valid_sqls]\n",
    "\n",
    "                    if pred_norm in valid_norms:\n",
    "                        correct += 1\n",
    "\n",
    "                    total += 1\n",
    "\n",
    "    sql_accuracy = correct / total if total > 0 else 0.0\n",
    "    template_accuracy = template_correct / total if total > 0 else 0.0\n",
    "    tag_accuracy = tag_correct / tag_total if tag_total > 0 else 0.0\n",
    "\n",
    "    print(f\"\\nFinal SQL Evaluation Accuracy: {sql_accuracy:.4f}\")\n",
    "    print(f\"Template Classification Accuracy: {template_accuracy:.4f}\")\n",
    "    print(f\"Tagging Accuracy: {tag_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Example 1 ---\n",
      "Original Input: what is the biggest city in kansas\n",
      "Predicted Template ID: 21\n",
      "SQL Template:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'state_name0', 'O']\n",
      "Tokens: ['what', 'is', 'the', 'biggest', 'city', 'in', 'kansas']\n",
      "Extracted Variables: {'state_name0': 'kansas'}\n",
      "\n",
      "Predicted Final SQL:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "\n",
      "Correct SQL Options:\n",
      "→ SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "✅ MATCHED\n",
      "\n",
      "--- Test Example 2 ---\n",
      "Original Input: what is the biggest city in louisiana\n",
      "Predicted Template ID: 21\n",
      "SQL Template:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'state_name0', 'O']\n",
      "Tokens: ['what', 'is', 'the', 'biggest', 'city', 'in', 'louisiana']\n",
      "Extracted Variables: {'state_name0': 'louisiana'}\n",
      "\n",
      "Predicted Final SQL:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "\n",
      "Correct SQL Options:\n",
      "→ SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "✅ MATCHED\n",
      "\n",
      "--- Test Example 3 ---\n",
      "Original Input: what is the largest city in california\n",
      "Predicted Template ID: 21\n",
      "SQL Template:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'state_name0', 'O']\n",
      "Tokens: ['what', 'is', 'the', 'largest', 'city', 'in', 'california']\n",
      "Extracted Variables: {'state_name0': 'california'}\n",
      "\n",
      "Predicted Final SQL:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "\n",
      "Correct SQL Options:\n",
      "→ SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "✅ MATCHED\n",
      "\n",
      "--- Test Example 4 ---\n",
      "Original Input: what is the largest city in rhode island\n",
      "Predicted Template ID: 21\n",
      "SQL Template:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'state_name0', 'O']\n",
      "Tokens: ['what', 'is', 'the', 'largest', 'city', 'in', 'rhode', 'island']\n",
      "Extracted Variables: {'state_name0': 'rhode'}\n",
      "\n",
      "Predicted Final SQL:\n",
      " SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "\n",
      "Correct SQL Options:\n",
      "→ SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "✅ MATCHED\n",
      "\n",
      "--- Test Example 5 ---\n",
      "Original Input: where is the most populated area of new mexico\n",
      "Predicted Template ID: 195\n",
      "SQL Template:\n",
      " SELECT STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"state_name0\" ;\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'state_name0']\n",
      "Tokens: ['where', 'is', 'the', 'most', 'populated', 'area', 'of', 'new']\n",
      "Extracted Variables: {'state_name0': 'new'}\n",
      "\n",
      "Predicted Final SQL:\n",
      " SELECT STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"state_name0\" ;\n",
      "\n",
      "Correct SQL Options:\n",
      "→ SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"state_name0\" ) AND CITYalias0.STATE_NAME = \"state_name0\" ;\n",
      "❌ NOT MATCHED\n",
      "\n",
      "Final SQL Evaluation Accuracy: 0.5842\n",
      "Template Classification Accuracy: 0.5842\n",
      "Tagging Accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader, question_split[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
