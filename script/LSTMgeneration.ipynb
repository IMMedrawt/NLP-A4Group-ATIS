{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your dataset\n",
    "with open(\"atis.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# with open(\"geography.json\", \"r\") as f:\n",
    "#     raw_data = json.load(f)\n",
    "\n",
    "# Load spaCy tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Structures to store splits and mappings\n",
    "question_split = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "query_split = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "template_pool = set()\n",
    "\n",
    "# Process each item, normalize gold SQLs to create template pool\n",
    "normalized_sqls_per_item = []\n",
    "for item in raw_data:\n",
    "    gold_templates = []\n",
    "    for sql in item[\"sql\"]:\n",
    "        template = sql\n",
    "        for var, val in item[\"sentences\"][0][\"variables\"].items():\n",
    "            template = template.replace(f'\"{val}\"', f'\"@{var}\"')\n",
    "            template = template.replace(val, f'\"@{var}\"')\n",
    "        gold_templates.append(template)\n",
    "        template_pool.add(template)\n",
    "    normalized_sqls_per_item.append((item, gold_templates))\n",
    "\n",
    "template_list = sorted(template_pool)\n",
    "template2id = {t: i for i, t in enumerate(template_list)}\n",
    "id2template = {i: t for t, i in template2id.items()}\n",
    "\n",
    "# Process entries and assign template IDs\n",
    "for (item, gold_templates) in normalized_sqls_per_item:\n",
    "    shortest_sql = sorted(gold_templates, key=lambda x: (len(x), x))[0]\n",
    "    template_id = template2id[shortest_sql]\n",
    "\n",
    "    for sent in item[\"sentences\"]:\n",
    "        raw_text = sent[\"text\"]\n",
    "        variables = sent[\"variables\"]\n",
    "\n",
    "        # Replace placeholders in raw_text for model input\n",
    "        text = raw_text\n",
    "        for var_name, var_value in variables.items():\n",
    "            text = text.replace(var_name, var_value)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = [t.text for t in nlp(text)]\n",
    "\n",
    "        # Create tag sequence\n",
    "        tags = []\n",
    "        for tok in tokens:\n",
    "            matched = False\n",
    "            for var_name, var_value in variables.items():\n",
    "                if tok == var_value or tok in var_value.split():\n",
    "                    tags.append(var_name)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                tags.append(\"O\")\n",
    "\n",
    "        # Fill in gold SQL (for seq2seq generation target)\n",
    "        gold_sql = shortest_sql\n",
    "        for var_name, var_value in variables.items():\n",
    "            gold_sql = gold_sql.replace(f\"@{var_name}\", f'\"{var_value}\"')\n",
    "\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"template_id\": template_id,\n",
    "            \"template_sql\": gold_templates,\n",
    "            \"variables\": variables,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"tokens\": tokens,\n",
    "            \"tags\": tags,\n",
    "            \"gold_sql\": gold_sql.strip(),\n",
    "        }\n",
    "\n",
    "        # Add to split\n",
    "        question_split[sent[\"question-split\"]].append(entry)\n",
    "        query_split[item[\"query-split\"]].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "tag_counter = Counter()\n",
    "\n",
    "for entry in question_split[\"train\"]:  \n",
    "    text = entry[\"text\"]\n",
    "    tokens = [t.text for t in nlp(text)]\n",
    "\n",
    "    tags = []\n",
    "    for tok in tokens:\n",
    "        matched = False\n",
    "        for var_name, var_val in entry[\"variables\"].items():\n",
    "            if tok == var_val or tok in var_val.split():\n",
    "                tags.append(var_name)\n",
    "                tag_counter[var_name] += 1\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            tags.append(\"O\")\n",
    "            tag_counter[\"O\"] += 1\n",
    "\n",
    "    token_counter.update(tokens)\n",
    "    entry[\"tokens\"] = tokens\n",
    "    entry[\"tags\"] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dev and test: use same logic, but do NOT update tag_counter/token_counter\n",
    "for split in [\"dev\", \"test\"]:\n",
    "    for entry in question_split[split]:\n",
    "        text = entry[\"text\"]\n",
    "        tokens = [t.text for t in nlp(text)]\n",
    "        tags = []\n",
    "        for tok in tokens:\n",
    "            matched = False\n",
    "            for var_name, var_val in entry[\"variables\"].items():\n",
    "                if tok == var_val or tok in var_val.split():\n",
    "                    tags.append(var_name)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                tags.append(\"O\")\n",
    "        entry[\"tokens\"] = tokens\n",
    "        entry[\"tags\"] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Token vocab\n",
    "token2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for token in token_counter:\n",
    "    token2id[token] = len(token2id)\n",
    "\n",
    "# Tag vocab \n",
    "PAD_TAG = \"<PAD>\"\n",
    "tag_counter[PAD_TAG] = 1  \n",
    "\n",
    "# tag2id = {}\n",
    "# for tag in tag_counter:\n",
    "#     tag2id[tag] = len(tag2id)\n",
    "\n",
    "tag2id = {\"<PAD>\": 0, \"O\": 1}\n",
    "for tag in tag_counter:\n",
    "    if tag not in tag2id:\n",
    "        tag2id[tag] = len(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, token2id, sql_token2id):\n",
    "        self.data = data\n",
    "        self.token2id = token2id\n",
    "        self.sql_token2id = sql_token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        # Encoder input\n",
    "        token_ids = [self.token2id.get(tok, self.token2id[\"<UNK>\"]) for tok in entry[\"tokens\"]]\n",
    "        # Decoder target (shortest SQL template filled with values)\n",
    "        sql = sorted(entry[\"template_sql\"], key=lambda x: (len(x), x))[0]\n",
    "        for var, val in entry[\"variables\"].items():\n",
    "            sql = sql.replace(f\"@{var}\", f'\"{val}\"')\n",
    "        sql_tokens = re.findall(r\"\\w+|[^\\s\\w]\", sql)\n",
    "        target_ids = [self.sql_token2id.get(tok, self.sql_token2id[\"<UNK>\"]) for tok in sql_tokens]\n",
    "\n",
    "        return {\n",
    "            \"token_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def pad_seq2seq_batch(batch):\n",
    "    max_input_len = max(len(item[\"token_ids\"]) for item in batch)\n",
    "    max_target_len = max(len(item[\"target_ids\"]) for item in batch)\n",
    "\n",
    "    for item in batch:\n",
    "        item[\"token_ids\"] = F.pad(item[\"token_ids\"], (0, max_input_len - len(item[\"token_ids\"])), value=token2id[\"<PAD>\"])\n",
    "        item[\"target_ids\"] = F.pad(item[\"target_ids\"], (0, max_target_len - len(item[\"target_ids\"])), value=sql_token2id[\"<PAD>\"])\n",
    "\n",
    "    token_ids = torch.stack([item[\"token_ids\"] for item in batch])\n",
    "    target_ids = torch.stack([item[\"target_ids\"] for item in batch])\n",
    "    return token_ids, target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Special tokens\n",
    "SQL_PAD = \"<PAD>\"\n",
    "SQL_UNK = \"<UNK>\"\n",
    "SQL_START = \"<START>\"\n",
    "SQL_END = \"<END>\"\n",
    "\n",
    "# Step 1: Collect all SQL tokens from training set\n",
    "sql_token_counter = Counter()\n",
    "\n",
    "for entry in question_split[\"train\"]:\n",
    "    sql = sorted(entry[\"template_sql\"], key=lambda x: (len(x), x))[0]\n",
    "    for var, val in entry[\"variables\"].items():\n",
    "        sql = sql.replace(f\"@{var}\", f'\"{val}\"')\n",
    "    # Basic tokenization: split by words and punctuation\n",
    "    sql_tokens = re.findall(r\"\\w+|[^\\s\\w]\", sql)\n",
    "    sql_token_counter.update(sql_tokens)\n",
    "\n",
    "# Step 2: Create vocab dict\n",
    "sql_token2id = {\n",
    "    SQL_PAD: 0,\n",
    "    SQL_UNK: 1,\n",
    "    SQL_START: 2,\n",
    "    SQL_END: 3\n",
    "}\n",
    "\n",
    "for token in sql_token_counter:\n",
    "    sql_token2id[token] = len(sql_token2id)\n",
    "\n",
    "# Step 3: Reverse map for inference (optional)\n",
    "id2sql_token = {i: tok for tok, i in sql_token2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_train_set = Seq2SeqDataset(question_split[\"train\"], token2id, sql_token2id)\n",
    "seq2seq_dev_set = Seq2SeqDataset(question_split[\"dev\"], token2id, sql_token2id)\n",
    "seq2seq_test_set = Seq2SeqDataset(question_split[\"test\"], token2id, sql_token2id)\n",
    "\n",
    "seq2seq_train_loader = DataLoader(seq2seq_train_set, batch_size=32, shuffle=True, collate_fn=pad_seq2seq_batch)\n",
    "seq2seq_dev_loader = DataLoader(seq2seq_dev_set, batch_size=32, collate_fn=pad_seq2seq_batch)\n",
    "seq2seq_test_loader = DataLoader(seq2seq_test_set, batch_size=32, collate_fn=pad_seq2seq_batch)\n",
    "seq2seq_query_test_set = Seq2SeqDataset(query_split[\"test\"], token2id, sql_token2id)\n",
    "seq2seq_query_test_loader = DataLoader(seq2seq_query_test_set, batch_size=32, collate_fn=pad_seq2seq_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2id[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq, lengths):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed)\n",
    "        return h_n, c_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=sql_token2id[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_seq, hidden):\n",
    "        embedded = self.embedding(target_seq)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        logits = self.out(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def step(self, input_token, hidden):\n",
    "        embedded = self.embedding(input_token)  # [B, 1, emb_dim]\n",
    "        output, hidden = self.lstm(embedded, hidden)  # [B, 1, hidden_dim]\n",
    "        logits = self.out(output.squeeze(1))  # [B, vocab_size]\n",
    "        return logits, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        outputs = torch.zeros(batch_size, trg_len, self.decoder.out.out_features).to(self.device)\n",
    "\n",
    "        lengths = (src != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "        h, c = self.encoder(src, lengths)\n",
    "\n",
    "        input_token = trg[:, 0].unsqueeze(1)  # <START>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, (h, c) = self.decoder(input_token, (h, c))\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            input_token = trg[:, t].unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = LSTMEncoder(len(token2id), embedding_dim=100, hidden_dim=128)\n",
    "decoder = LSTMDecoder(len(sql_token2id), embedding_dim=100, hidden_dim=128)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sql_counter = Counter()\n",
    "sql_sentences = []\n",
    "\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    for entry in question_split[split]:\n",
    "        # Use the shortest SQL template\n",
    "        sql = sorted(entry[\"template_sql\"], key=lambda x: (len(x), x))[0]\n",
    "\n",
    "        # Replace variables with values\n",
    "        for var, val in entry[\"variables\"].items():\n",
    "            sql = sql.replace(f'\"@{var}\"', f'\"{val}\"').replace(f\"@{var}\", f'\"{val}\"')\n",
    "\n",
    "        tokens = re.findall(r\"\\w+|[^\\s\\w]\", sql)  # basic tokenizer\n",
    "        sql_counter.update(tokens)\n",
    "        sql_sentences.append(tokens)\n",
    "        entry[\"sql_tokens\"] = tokens  # Save for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<PAD>\", \"<UNK>\", \"<START>\", \"<END>\"]\n",
    "sql_token2id = {tok: idx for idx, tok in enumerate(SPECIAL_TOKENS)}\n",
    "for tok in sql_counter:\n",
    "    if tok not in sql_token2id:\n",
    "        sql_token2id[tok] = len(sql_token2id)\n",
    "\n",
    "id2sql_token = {v: k for k, v in sql_token2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sql(tokens):\n",
    "    return [sql_token2id[\"<START>\"]] + \\\n",
    "           [sql_token2id.get(tok, sql_token2id[\"<UNK>\"]) for tok in tokens] + \\\n",
    "           [sql_token2id[\"<END>\"]]\n",
    "\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    for entry in question_split[split]:\n",
    "        entry[\"sql_ids\"] = encode_sql(entry[\"sql_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=sql_token2id[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_seq2seq(model, loader, loss_fn, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for token_ids, target_ids in loader:\n",
    "        token_ids = token_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(token_ids, target_ids[:, :-1])  # input is target w/o <END>\n",
    "        loss = loss_fn(output.view(-1, output.shape[-1]), target_ids[:, 1:].reshape(-1))  # shift target\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_seq2seq(model, loader, loss_fn, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token_ids, target_ids in loader:\n",
    "            token_ids = token_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            output = model(token_ids, target_ids[:, :-1])  # input is target w/o <END>\n",
    "            loss = loss_fn(output.view(-1, output.shape[-1]), target_ids[:, 1:].reshape(-1))  # shift target\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"[Dev] Epoch {epoch}: Loss={avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.9030\n",
      "[Dev] Epoch 1: Loss=0.6673\n",
      "Epoch 2: Loss=0.6125\n",
      "[Dev] Epoch 2: Loss=0.4993\n",
      "Epoch 3: Loss=0.4798\n",
      "[Dev] Epoch 3: Loss=0.4112\n",
      "Epoch 4: Loss=0.4042\n",
      "[Dev] Epoch 4: Loss=0.3605\n",
      "Epoch 5: Loss=0.3573\n",
      "[Dev] Epoch 5: Loss=0.3263\n",
      "Epoch 6: Loss=0.3239\n",
      "[Dev] Epoch 6: Loss=0.2997\n",
      "Epoch 7: Loss=0.2968\n",
      "[Dev] Epoch 7: Loss=0.2744\n",
      "Epoch 8: Loss=0.2756\n",
      "[Dev] Epoch 8: Loss=0.2607\n",
      "Epoch 9: Loss=0.2558\n",
      "[Dev] Epoch 9: Loss=0.2473\n",
      "Epoch 10: Loss=0.2404\n",
      "[Dev] Epoch 10: Loss=0.2389\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_seq2seq(model, seq2seq_train_loader, loss_fn, optimizer, epoch)\n",
    "    validate_seq2seq(model, seq2seq_dev_loader, loss_fn, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sql(s):\n",
    "    return \" \".join(s.strip().split()).lower()\n",
    "\n",
    "\n",
    "def generate_sql_from_model(model, loader, output_file=\"seq2seq_test_predictions.txt\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    golds = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            for token_ids, target_ids in loader:\n",
    "                token_ids = token_ids.to(device)\n",
    "                lengths = (token_ids != token2id[\"<PAD>\"]).sum(dim=1)\n",
    "\n",
    "                batch_size = token_ids.size(0)\n",
    "\n",
    "                # Encode\n",
    "                h, c = model.encoder(token_ids, lengths)\n",
    "\n",
    "                input_token = torch.full((batch_size, 1), sql_token2id[\"<START>\"], dtype=torch.long, device=device)\n",
    "                outputs = []\n",
    "\n",
    "                for _ in range(100):\n",
    "                    logits, (h, c) = model.decoder.step(input_token, (h, c))  \n",
    "                    pred_token = logits.argmax(dim=-1, keepdim=True)         \n",
    "                    outputs.append(pred_token)\n",
    "                    input_token = pred_token\n",
    "\n",
    "\n",
    "                output_seqs = torch.cat(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_tokens = output_seqs[i].tolist()\n",
    "                    if sql_token2id[\"<END>\"] in pred_tokens:\n",
    "                        pred_tokens = pred_tokens[:pred_tokens.index(sql_token2id[\"<END>\"])]\n",
    "                    pred_sql = normalize_sql(\" \".join(id2sql_token[tok] for tok in pred_tokens))\n",
    "\n",
    "                    gold_tokens = target_ids[i].tolist()\n",
    "                    if sql_token2id[\"<END>\"] in gold_tokens:\n",
    "                        gold_tokens = gold_tokens[:gold_tokens.index(sql_token2id[\"<END>\"])]\n",
    "                    gold_sql = normalize_sql(\" \".join(id2sql_token[tok] for tok in gold_tokens))\n",
    "\n",
    "                    predictions.append(pred_sql)\n",
    "                    golds.append(gold_sql)\n",
    "                    f.write(pred_sql + \"\\n\")\n",
    "\n",
    "                    if pred_sql == gold_sql:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Test Seq2Seq] SQL Generation Accuracy: {acc:.4f}\")\n",
    "    return predictions, golds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Seq2Seq] SQL Generation Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "preds, golds = generate_sql_from_model(model, seq2seq_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation on Question Split]\n",
      "[Test Seq2Seq] SQL Generation Accuracy: 0.0000\n",
      "[Evaluation on Query Split]\n",
      "[Test Seq2Seq] SQL Generation Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"[Evaluation on Question Split]\")\n",
    "preds_question, golds_question = generate_sql_from_model(model, seq2seq_test_loader, output_file=\"seq2seq_question_preds.txt\")\n",
    "\n",
    "print(\"[Evaluation on Query Split]\")\n",
    "preds_query, golds_query = generate_sql_from_model(model, seq2seq_query_test_loader, output_file=\"seq2seq_query_preds.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
